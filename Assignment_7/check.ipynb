{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a186b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "\n",
    "PICKLE_PATH = r\"gram_models.pkl\"\n",
    "from ngram_model_def import NGramLanguageModel\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        with open(PICKLE_PATH, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print('ERROR loading pickle:', e)\n",
    "        sys.exit(2)\n",
    "\n",
    "    print('Loaded object type:', type(data))\n",
    "    if hasattr(data, 'keys'):\n",
    "        keys = list(data.keys())\n",
    "        print('Top-level keys:', keys)\n",
    "    else:\n",
    "        print('Top-level object is not a dict; repr:', repr(data)[:200])\n",
    "        return\n",
    "\n",
    "    for k in keys:\n",
    "        v = data[k]\n",
    "        try:\n",
    "            l = len(v)\n",
    "        except Exception:\n",
    "            l = 'N/A'\n",
    "        print('\\nKEY:', k, 'type:', type(v), 'len:', l)\n",
    "        if isinstance(v, dict):\n",
    "            print('  sample entries:')\n",
    "            for i, (kk, vv) in enumerate(v.items()):\n",
    "                if i >= 5:\n",
    "                    break\n",
    "                t = type(vv)\n",
    "                try:\n",
    "                    lv = len(vv)\n",
    "                except Exception:\n",
    "                    lv = 'N/A'\n",
    "                print('   ', i, 'key=', kk, '-> type=', t, 'len=', lv)\n",
    "        else:\n",
    "            # try to show a small sample repr\n",
    "            try:\n",
    "                print('  repr sample:', repr(v)[:200])\n",
    "            except Exception:\n",
    "                pass\n",
    "            # Print NGramLanguageModel summary if applicable\n",
    "            if isinstance(v, NGramLanguageModel):\n",
    "                print('  NGramLanguageModel summary:')\n",
    "                print('    n:', getattr(v, 'n', 'N/A'))\n",
    "                ngram_counts = getattr(v, 'ngram_counts', None)\n",
    "                if ngram_counts is not None:\n",
    "                    print('    ngram_counts entries:', len(ngram_counts))\n",
    "                else:\n",
    "                    print('    ngram_counts: not found')\n",
    "\n",
    "    print('\\nINSPECTION COMPLETE')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from math import log10\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, n=1):\n",
    "        self.n = n\n",
    "        self.ngram_counts = Counter()\n",
    "        self.context_counts = Counter()\n",
    "        self.vocabulary = set()\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def add_sentence_markers(self, sentence):\n",
    "        return ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
    "\n",
    "    def get_ngrams(self, sentence):\n",
    "        marked_sentence = self.add_sentence_markers(sentence)\n",
    "        ngrams = []\n",
    "        for i in range(len(marked_sentence) - self.n + 1):\n",
    "            ngram = tuple(marked_sentence[i:i + self.n])\n",
    "            ngrams.append(ngram)\n",
    "        return ngrams\n",
    "\n",
    "    def train(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            self.vocabulary.update(sentence)\n",
    "        self.vocabulary.add('<s>')\n",
    "        self.vocabulary.add('</s>')\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        for sentence in sentences:\n",
    "            ngrams = self.get_ngrams(sentence)\n",
    "            for ngram in ngrams:\n",
    "                self.ngram_counts[ngram] += 1\n",
    "                if self.n > 1:\n",
    "                    context = ngram[:-1]\n",
    "                    self.context_counts[context] += 1\n",
    "\n",
    "    def probability(self, ngram, smoothing='none', k=1):\n",
    "        if isinstance(ngram, list):\n",
    "            ngram = tuple(ngram)\n",
    "        if self.n == 1:\n",
    "            count = self.ngram_counts[ngram]\n",
    "            total = sum(self.ngram_counts.values())\n",
    "            if smoothing == 'add_one':\n",
    "                return (count + 1) / (total + self.vocab_size)\n",
    "            elif smoothing == 'add_k':\n",
    "                return (count + k) / (total + k * self.vocab_size)\n",
    "            elif smoothing == 'add_token_type':\n",
    "                unique_types = len(self.ngram_counts)\n",
    "                return (count + 1) / (total + unique_types)\n",
    "            else:\n",
    "                return count / total if total > 0 else 0\n",
    "        else:\n",
    "            context = ngram[:-1]\n",
    "            count = self.ngram_counts[ngram]\n",
    "            context_count = self.context_counts[context]\n",
    "            if smoothing == 'add_one':\n",
    "                return (count + 1) / (context_count + self.vocab_size)\n",
    "            elif smoothing == 'add_k':\n",
    "                return (count + k) / (context_count + k * self.vocab_size)\n",
    "            elif smoothing == 'add_token_type':\n",
    "                unique_types = len([ng for ng in self.ngram_counts.keys() if ng[:-1] == context])\n",
    "                if unique_types == 0:\n",
    "                    unique_types = 1\n",
    "                return (count + 1) / (context_count + unique_types)\n",
    "            else:\n",
    "                return count / context_count if context_count > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3490cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import math\n",
    "import csv\n",
    "from ngram_model_def import NGramLanguageModel\n",
    "\n",
    "TRAIN_PATH = \"train.txt\"\n",
    "VAL_PATH = \"val.txt\"\n",
    "TEST_PATH = \"test.txt\"\n",
    "PICKLE_PATH = Path(__file__).with_name('ngram_models.pkl')\n",
    "OUTPUT_CSV = Path(__file__).with_name('pmi_top100.csv')\n",
    "MIN_COUNT = 5\n",
    "\n",
    "def load_models(pickle_path):\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        models = pickle.load(f)\n",
    "    return models\n",
    "\n",
    "def get_counts(models):\n",
    "    unigram = models.get('Unigram')\n",
    "    bigram = models.get('Bigram')\n",
    "    uni_counts = unigram.ngram_counts\n",
    "    bi_counts = bigram.ngram_counts\n",
    "    total_unigrams = sum(uni_counts.values())\n",
    "    total_bigrams = sum(bi_counts.values())\n",
    "    return uni_counts, bi_counts, total_unigrams, total_bigrams\n",
    "\n",
    "def compute_pmi(uni_counts, bi_counts, total_unigrams, total_bigrams, min_count=MIN_COUNT):\n",
    "    # count upon total formula use kairi che for probablities\n",
    "    results = []\n",
    "    for (w1, w2), c_xy in bi_counts.items():\n",
    "        if c_xy < min_count:\n",
    "            continue # skip low count bigrams to reduce noise\n",
    "        c_x = uni_counts.get((w1,), 0)\n",
    "        c_y = uni_counts.get((w2,), 0)\n",
    "        if c_x == 0 or c_y == 0:\n",
    "            continue\n",
    "        p_x = c_x / total_unigrams\n",
    "        p_y = c_y / total_unigrams\n",
    "        p_xy = c_xy / total_bigrams\n",
    "        denom = p_x * p_y\n",
    "        if denom == 0 or p_xy == 0:\n",
    "            continue\n",
    "        pmi = math.log(p_xy / denom)\n",
    "        results.append((w1, w2, c_xy, c_x, c_y, p_x, p_y, p_xy, pmi))\n",
    "\n",
    "    results.sort(key=lambda x: x[-1], reverse=True)\n",
    "    return results\n",
    "\n",
    "def write_csv(results, out_path, top_k=100):\n",
    "    headers = ['word1', 'word2', 'count_bigram', 'count_w1', 'count_w2', 'p_w1', 'p_w2', 'p_w1w2', 'pmi']\n",
    "    with open(out_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)\n",
    "        for row in results[:top_k]:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def read_sentences(filepath):\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        return [line.strip().split() for line in f if line.strip()]\n",
    "\n",
    "def extract_bigrams_from_sentences(sentences):\n",
    "    bigrams = []\n",
    "    for tokens in sentences:\n",
    "        bigrams.extend([(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)])\n",
    "    return bigrams\n",
    "\n",
    "def compute_pmi_for_bigrams(bigrams, uni_counts, bi_counts, total_unigrams, total_bigrams):\n",
    "    pmi_scores = []\n",
    "    for w1, w2 in bigrams:\n",
    "        c_xy = bi_counts.get((w1, w2), 0)\n",
    "        c_x = uni_counts.get((w1,), 0)\n",
    "        c_y = uni_counts.get((w2,), 0)\n",
    "        if c_x == 0 or c_y == 0 or c_xy == 0:\n",
    "            pmi = None\n",
    "        else:\n",
    "            p_x = c_x / total_unigrams\n",
    "            p_y = c_y / total_unigrams\n",
    "            p_xy = c_xy / total_bigrams\n",
    "            denom = p_x * p_y\n",
    "            if denom == 0 or p_xy == 0:\n",
    "                pmi = None\n",
    "            else:\n",
    "                pmi = math.log(p_xy / denom)\n",
    "        pmi_scores.append((w1, w2, c_xy, c_x, c_y, pmi))\n",
    "    return pmi_scores\n",
    "\n",
    "def print_pmi_scores(pmi_scores, label, top_k=10):\n",
    "    filtered = [row for row in pmi_scores if row[-1] is not None]\n",
    "    filtered.sort(key=lambda x: x[-1], reverse=True)\n",
    "    print(f\"\\nTop {top_k} PMI bigrams in {label}:\")\n",
    "    for i, row in enumerate(filtered[:top_k], 1):\n",
    "        w1, w2, c_xy, c_x, c_y, pmi = row\n",
    "        print(f\"{i:2d}. {w1} {w2}  PMI={pmi:.4f}  count={c_xy}\")\n",
    "\n",
    "print('Loading models from', PICKLE_PATH)\n",
    "models = load_models(PICKLE_PATH)\n",
    "# Print summary for Unigram and Bigram models\n",
    "for key in ['Unigram', 'Bigram']:\n",
    "    model = models.get(key)\n",
    "    if model is not None:\n",
    "        print(f\"{key} model: n={getattr(model, 'n', 'N/A')}, ngram_counts entries={len(getattr(model, 'ngram_counts', {}))}\")\n",
    "uni_counts, bi_counts, total_unigrams, total_bigrams = get_counts(models)\n",
    "print('Total unigrams:', total_unigrams, 'Total bigrams:', total_bigrams)\n",
    "results = compute_pmi(uni_counts, bi_counts, total_unigrams, total_bigrams)\n",
    "if not results:\n",
    "    print('No bigrams passed the minimum count threshold')\n",
    "    exit(0)\n",
    "write_csv(results, OUTPUT_CSV)\n",
    "print(f'Wrote top {min(100, len(results))} PMI bigrams to', OUTPUT_CSV)\n",
    "print('\\nTop 10 PMI bigrams:')\n",
    "for i, r in enumerate(results[:10], 1):\n",
    "    w1, w2, c_xy, c_x, c_y, p_x, p_y, p_xy, pmi = r\n",
    "    print(f\"{i:2d}. {w1} {w2}  PMI={pmi:.4f}  count={c_xy}\")\n",
    "\n",
    "train_sentences = read_sentences(TRAIN_PATH)\n",
    "unigram_model = NGramLanguageModel(n=1)\n",
    "bigram_model = NGramLanguageModel(n=2)\n",
    "unigram_model.train(train_sentences)\n",
    "bigram_model.train(train_sentences)\n",
    "uni_counts = unigram_model.ngram_counts\n",
    "bi_counts = bigram_model.ngram_counts\n",
    "total_unigrams = sum(uni_counts.values())\n",
    "total_bigrams = sum(bi_counts.values())\n",
    "\n",
    "for split, path in [('val', VAL_PATH), ('test', TEST_PATH)]:\n",
    "    sentences = read_sentences(path)\n",
    "    bigrams = extract_bigrams_from_sentences(sentences)\n",
    "    pmi_scores = compute_pmi_for_bigrams(bigrams, uni_counts, bi_counts, total_unigrams, total_bigrams)\n",
    "    print_pmi_scores(pmi_scores, split, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19716ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
