{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5855c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_bpe_model():\n",
    "    with open('bpe_model.json', 'r', encoding='utf-8') as f:\n",
    "        model = json.load(f)\n",
    "    \n",
    "    print(\"BPE MODEL ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total vocabulary size: {len(model['vocab'])}\")\n",
    "    print(f\"Total merges performed: {len(model['merges'])}\")\n",
    "    \n",
    "    # analyze merge patterns\n",
    "    merge_lengths = []\n",
    "    for pair in model['merges']:\n",
    "        merged = ''.join(pair)\n",
    "        merge_lengths.append(len(merged))\n",
    "    \n",
    "    print(f\"Average merge length: {sum(merge_lengths)/len(merge_lengths):.2f}\")\n",
    "    print(f\"Max merge length: {max(merge_lengths)}\")\n",
    "    \n",
    "    # show some sample merges\n",
    "    print(\"\\nFirst 20 merges:\")\n",
    "    for i, pair in enumerate(model['merges'][:20]):\n",
    "        print(f\"  {i+1:2d}. {pair[0]} + {pair[1]} = {''.join(pair)}\")\n",
    "    \n",
    "    print(\"\\nLast 20 merges:\")\n",
    "    for i, pair in enumerate(model['merges'][-20:], len(model['merges'])-19):\n",
    "        print(f\"  {i:2d}. {pair[0]} + {pair[1]} = {''.join(pair)}\")\n",
    "    \n",
    "    # analyze vocabulary\n",
    "    vocab_lengths = [len(token.replace('</w>', '')) for token in model['vocab']]\n",
    "    print(f\"\\nVocabulary token lengths:\")\n",
    "    print(f\"  Average: {sum(vocab_lengths)/len(vocab_lengths):.2f}\")\n",
    "    print(f\"  Max: {max(vocab_lengths)}\")\n",
    "    print(f\"  Min: {min(vocab_lengths)}\")\n",
    "    \n",
    "    # show longest tokens\n",
    "    longest_tokens = sorted(model['vocab'], key=lambda x: len(x.replace('</w>', '')), reverse=True)[:10]\n",
    "    print(f\"\\nLongest BPE tokens:\")\n",
    "    for i, token in enumerate(longest_tokens, 1):\n",
    "        print(f\"  {i:2d}. {token} (length: {len(token.replace('</w>', ''))})\")\n",
    "\n",
    "def analyze_wordpiece_model():\n",
    "    with open('wordpiece_model.json', 'r', encoding='utf-8') as f:\n",
    "        model = json.load(f)\n",
    "    \n",
    "    print(\"\\n\\nWORDPIECE MODEL ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total vocabulary size: {len(model['vocab'])}\")\n",
    "    print(f\"Total merges performed: {len(model['merges'])}\")\n",
    "    \n",
    "    # analyze merge patterns\n",
    "    merge_lengths = []\n",
    "    for (pair, new_sym) in model['merges']:\n",
    "        merge_lengths.append(len(new_sym.replace('##', '')))\n",
    "    \n",
    "    print(f\"Average merged token length: {sum(merge_lengths)/len(merge_lengths):.2f}\")\n",
    "    print(f\"Max merged token length: {max(merge_lengths)}\")\n",
    "    \n",
    "    # show some sample merges\n",
    "    print(\"\\nFirst 20 merges:\")\n",
    "    for i, ((s1, s2), new_sym) in enumerate(model['merges'][:20]):\n",
    "        print(f\"  {i+1:2d}. {s1} + {s2} = {new_sym}\")\n",
    "    \n",
    "    print(\"\\nLast 20 merges:\")\n",
    "    for i, ((s1, s2), new_sym) in enumerate(model['merges'][-20:], len(model['merges'])-19):\n",
    "        print(f\"  {i:2d}. {s1} + {s2} = {new_sym}\")\n",
    "    \n",
    "    # analyze vocabulary\n",
    "    vocab_lengths = [len(token.replace('##', '')) for token in model['vocab']]\n",
    "    print(f\"\\nVocabulary token lengths:\")\n",
    "    print(f\"  Average: {sum(vocab_lengths)/len(vocab_lengths):.2f}\")\n",
    "    print(f\"  Max: {max(vocab_lengths)}\")\n",
    "    print(f\"  Min: {min(vocab_lengths)}\")\n",
    "    \n",
    "    # show longest tokens\n",
    "    longest_tokens = sorted(model['vocab'], key=lambda x: len(x.replace('##', '')), reverse=True)[:10]\n",
    "    print(f\"\\nLongest WordPiece tokens:\")\n",
    "    for i, token in enumerate(longest_tokens, 1):\n",
    "        print(f\"  {i:2d}. {token} (length: {len(token.replace('##', ''))})\")\n",
    "    \n",
    "    # count ## tokens vs regular tokens\n",
    "    continuation_tokens = sum(1 for token in model['vocab'] if token.startswith('##'))\n",
    "    regular_tokens = len(model['vocab']) - continuation_tokens\n",
    "    print(f\"\\nToken types:\")\n",
    "    print(f\"  Regular tokens: {regular_tokens}\")\n",
    "    print(f\"  Continuation tokens (##): {continuation_tokens}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_bpe_model()\n",
    "    analyze_wordpiece_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "MERGE_STEPS = 32000\n",
    "VOCAB_SIZE = 32000\n",
    "GUJARATI_CHARS = set(range(0x0A80, 0x0B00))\n",
    "GUJARATI_MATRAS = set([0x0ABE, 0x0ABF, 0x0AC0, 0x0AC1, 0x0AC2, 0x0AC3, 0x0AC4, 0x0AC5, 0x0AC7, 0x0AC8, 0x0AC9, 0x0ACB, 0x0ACC])\n",
    "GUJARATI_VIRAMAS = set([0x0ACD])\n",
    "\n",
    "def gujarati_tokenize(text):\n",
    "    tokens = []\n",
    "    current_word = []\n",
    "    \n",
    "    for char in text:\n",
    "        char_code = ord(char)\n",
    "        \n",
    "        if char_code in GUJARATI_CHARS:\n",
    "            current_word.append(char)\n",
    "        elif char.isspace(): # space aave to tokens ma add kari devo \n",
    "            if current_word:\n",
    "                tokens.append(''.join(current_word))\n",
    "                current_word = []\n",
    "        elif char.isalnum():\n",
    "            current_word.append(char)\n",
    "        else:\n",
    "            if current_word:\n",
    "                tokens.append(''.join(current_word))\n",
    "                current_word = []\n",
    "    \n",
    "    if current_word: # last ma to add karvanuj che\n",
    "        tokens.append(''.join(current_word))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def split_gujarati_word(word):\n",
    "    chars = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        char = word[i]\n",
    "        \n",
    "        chars.append(char)\n",
    "        i += 1\n",
    "        \n",
    "        while i < len(word):\n",
    "            next_char = word[i]\n",
    "            next_code = ord(next_char)\n",
    "            if next_code in GUJARATI_MATRAS or next_code in GUJARATI_VIRAMAS:\n",
    "                chars[-1] += next_char\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    return chars\n",
    "\n",
    "def read_corpus(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def get_word_freqs(corpus):\n",
    "    word_freqs = Counter()\n",
    "    for line in corpus:\n",
    "        words = gujarati_tokenize(line.lower())\n",
    "        for word in words:\n",
    "            if any(ord(c) in GUJARATI_CHARS for c in word):\n",
    "                word_freqs[word] += 1\n",
    "    return word_freqs\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = Counter()\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, vocab): # pair is tuple of 2 symbols\n",
    "    new_vocab = {}\n",
    "    bigram = re.escape(' '.join(pair)) # space thi join kairi, used escape for handling special chars\n",
    "    #not preceded by a non-space character and not followed by a non-space character\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in vocab:\n",
    "        new_word = p.sub(''.join(pair), word)\n",
    "        new_vocab[new_word] = vocab[word]\n",
    "    return new_vocab\n",
    "\n",
    "# read data\n",
    "corpus = read_corpus('train_sampled.txt')\n",
    "print(f\"Corpus size: {len(corpus)} sentences\")\n",
    "word_freqs = get_word_freqs(corpus)\n",
    "vocab = {}\n",
    "for word, freq in word_freqs.items():\n",
    "    chars = split_gujarati_word(word)\n",
    "    vocab[' '.join(chars) + ' </w>'] = freq\n",
    "\n",
    "print(f\"Initial vocab size: {len(set(' '.join(vocab.keys()).split()))}\")\n",
    "\n",
    "merges = []\n",
    "for i in range(MERGE_STEPS):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Merge {i}/{MERGE_STEPS}\")\n",
    "    \n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        print(f\"No more pairs to merge at step {i}\")\n",
    "        break\n",
    "    \n",
    "    # BPE: select most frequent pair (frequency-based)\n",
    "    best_pair = pairs.most_common(1)[0][0]\n",
    "    vocab = merge_vocab(best_pair, vocab)\n",
    "    merges.append(best_pair)\n",
    "    \n",
    "    # check vocabulary size\n",
    "    all_symbols = set(' '.join(vocab.keys()).split())\n",
    "    if len(all_symbols) >= VOCAB_SIZE:\n",
    "        print(f\"Reached vocab size {len(all_symbols)} at step {i}\")\n",
    "        break\n",
    "\n",
    "print(f\"Training completed. Total merges: {len(merges)}\")\n",
    "\n",
    "# save model\n",
    "final_vocab = set(' '.join(vocab.keys()).split())\n",
    "model_data = {\n",
    "    'vocab': list(final_vocab),\n",
    "    'merges': merges,\n",
    "    'word_vocab': vocab\n",
    "}\n",
    "\n",
    "with open('bpe_model.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_data, f, ensure_ascii=False)\n",
    "\n",
    "def bpe_encode(text, merges):\n",
    "    words = gujarati_tokenize(text.lower())\n",
    "    encoded = []\n",
    "    \n",
    "    for word in words:\n",
    "        if not any(ord(c) in GUJARATI_CHARS for c in word):\n",
    "            continue\n",
    "        chars = split_gujarati_word(word)\n",
    "        word_tokens = chars + ['</w>']\n",
    "        word_str = ' '.join(word_tokens)\n",
    "        \n",
    "        for pair in merges:\n",
    "            bigram = re.escape(' '.join(pair))\n",
    "            p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "            word_str = p.sub(''.join(pair), word_str)\n",
    "        \n",
    "        encoded.extend(word_str.split())\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "# test encoding\n",
    "test_sentences = [\n",
    "    \"જિલ્લા ફોરમમાં, જેની હકૂમતની અંદર.\",\n",
    "    \"તમારા એમ્પ્લોયરનું નામ, તમારા કાર્યાલયનું સરનામું.\",\n",
    "    \"ભાઈ, જુઓ છો ને!\"\n",
    "]\n",
    "\n",
    "print(f\"\\nFinal vocab size: {len(final_vocab)}\")\n",
    "print(f\"Number of merges: {len(merges)}\")\n",
    "\n",
    "print(\"\\nBPE Encoding Examples:\")\n",
    "for sentence in test_sentences:\n",
    "    encoded = bpe_encode(sentence, merges)\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Encoded: {encoded}\")\n",
    "    print()\n",
    "\n",
    "print(\"Model saved to bpe_model.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56438bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "ITR = 32000\n",
    "VOCAB_SIZE = 32000\n",
    "test_text = \"છોકરો બિલાડી સાથે રમે છે અને કૂતરો બગીચામાં દોડે છે\"\n",
    "\n",
    "GUJARATI_CHARS = set(range(0x0A80, 0x0B00))\n",
    "GUJARATI_MATRAS = set([0x0ABE, 0x0ABF, 0x0AC0, 0x0AC1, 0x0AC2, 0x0AC3, 0x0AC4, 0x0AC5, 0x0AC7, 0x0AC8, 0x0AC9, 0x0ACB, 0x0ACC])\n",
    "GUJARATI_VIRAMAS = set([0x0ACD])\n",
    "\n",
    "def gujarati_tokenize(text):\n",
    "    tokens = []\n",
    "    current_word = []\n",
    "    \n",
    "    for char in text:\n",
    "        char_code = ord(char)\n",
    "        \n",
    "        if char_code in GUJARATI_CHARS:\n",
    "            current_word.append(char)\n",
    "        elif char.isspace():\n",
    "            if current_word:\n",
    "                tokens.append(''.join(current_word))\n",
    "                current_word = []\n",
    "        elif char.isalnum():\n",
    "            current_word.append(char)\n",
    "        else:\n",
    "            if current_word:\n",
    "                tokens.append(''.join(current_word))\n",
    "                current_word = []\n",
    "    \n",
    "    if current_word:\n",
    "        tokens.append(''.join(current_word))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def split_gujarati_word(word):\n",
    "    chars = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        char = word[i]\n",
    "        char_code = ord(char)\n",
    "        \n",
    "        chars.append(char)\n",
    "        i += 1\n",
    "        \n",
    "        while i < len(word):\n",
    "            next_char = word[i]\n",
    "            next_code = ord(next_char)\n",
    "            if next_code in GUJARATI_MATRAS or next_code in GUJARATI_VIRAMAS:\n",
    "                chars[-1] += next_char\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    return chars\n",
    "\n",
    "def read_sentences(filepath):\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        return [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "input_sentences = read_sentences(\"train_sampled.txt\")\n",
    "\n",
    "# collect tokens from corpus\n",
    "tokens = []\n",
    "for sentence in input_sentences:\n",
    "    sentence_tokens = gujarati_tokenize(sentence)\n",
    "    for token in sentence_tokens:\n",
    "        if any(ord(c) in GUJARATI_CHARS for c in token):\n",
    "            tokens.append(token)\n",
    "\n",
    "# build word frequencies\n",
    "word_freqs = Counter(tokens)\n",
    "\n",
    "# build initial symbol lists for each distinct word\n",
    "word_symbols = {}\n",
    "for w in set(tokens):\n",
    "    if not w:\n",
    "        continue\n",
    "    chars = split_gujarati_word(w)\n",
    "    if len(chars) == 1:\n",
    "        word_symbols[w] = [chars[0]]\n",
    "    else:\n",
    "        word_symbols[w] = [chars[0]] + [f\"##{c}\" for c in chars[1:]]\n",
    "\n",
    "def get_pair_counts(word_symbols, word_freqs):\n",
    "    pairs = Counter()\n",
    "    for word, symbols in word_symbols.items():\n",
    "        freq = word_freqs[word]\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def find_best_merge(pairs, word_symbols, word_freqs):\n",
    "    # WordPiece uses probability-based scoring: P(xy) / (P(x) * P(y))\n",
    "    symbol_counts = Counter()\n",
    "    \n",
    "    # count individual symbols weighted by word frequency\n",
    "    for word, symbols in word_symbols.items():\n",
    "        freq = word_freqs[word]\n",
    "        for symbol in symbols:\n",
    "            symbol_counts[symbol] += freq\n",
    "    \n",
    "    total_pairs = sum(pairs.values())\n",
    "    total_symbols = sum(symbol_counts.values())\n",
    "    \n",
    "    best_pair = None\n",
    "    best_score = float('-inf')\n",
    "    \n",
    "    for (x, y), freq in pairs.items():\n",
    "        if freq > 1:  # only consider pairs that appear more than once\n",
    "            p_xy = freq / total_pairs if total_pairs > 0 else 0\n",
    "            p_x = symbol_counts[x] / total_symbols if total_symbols > 0 else 0\n",
    "            p_y = symbol_counts[y] / total_symbols if total_symbols > 0 else 0\n",
    "            \n",
    "            # WordPiece probability score: P(xy) / (P(x) * P(y))\n",
    "            if p_x > 0 and p_y > 0:\n",
    "                score = p_xy / (p_x * p_y)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_pair = (x, y)\n",
    "    \n",
    "    return best_pair, best_score\n",
    "\n",
    "def merge_symbols(pair, word_symbols):\n",
    "    symbol1, symbol2 = pair\n",
    "    y_clean = symbol2[2:] if symbol2.startswith('##') else symbol2\n",
    "    new_symbol = symbol1 + y_clean\n",
    "    \n",
    "    new_word_symbols = {}\n",
    "    for word, symbols in word_symbols.items():\n",
    "        new_symbols = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols) - 1 and symbols[i] == symbol1 and symbols[i + 1] == symbol2:\n",
    "                new_symbols.append(new_symbol)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        new_word_symbols[word] = new_symbols\n",
    "    \n",
    "    return new_word_symbols, new_symbol\n",
    "\n",
    "# perform merges using WordPiece probability scoring\n",
    "vocab_symbols = set()\n",
    "for symbols in word_symbols.values():\n",
    "    vocab_symbols.update(symbols)\n",
    "\n",
    "print(f\"Initial vocab size: {len(vocab_symbols)}\")\n",
    "\n",
    "merged_pairs = []\n",
    "for i in range(ITR):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Merge {i}/{ITR}, vocab size: {len(vocab_symbols)}\")\n",
    "    \n",
    "    pairs = get_pair_counts(word_symbols, word_freqs)\n",
    "    if not pairs:\n",
    "        print(f\"No more pairs to merge at step {i}\")\n",
    "        break\n",
    "    \n",
    "    if len(vocab_symbols) >= VOCAB_SIZE:\n",
    "        print(f\"Reached vocab size {len(vocab_symbols)} at step {i}\")\n",
    "        break\n",
    "    \n",
    "    # WordPiece: select pair with highest probability score\n",
    "    best_pair, score = find_best_merge(pairs, word_symbols, word_freqs)\n",
    "    if best_pair is None:\n",
    "        print(f\"No valid pair found at step {i}\")\n",
    "        break\n",
    "    \n",
    "    word_symbols, new_symbol = merge_symbols(best_pair, word_symbols)\n",
    "    merged_pairs.append((best_pair, new_symbol))\n",
    "    vocab_symbols.add(new_symbol)\n",
    "\n",
    "print(f\"Training completed. Total merges: {len(merged_pairs)}\")\n",
    "\n",
    "final_vocab = vocab_symbols\n",
    "\n",
    "# save model\n",
    "model_data = {\n",
    "    'vocab': list(final_vocab),\n",
    "    'merges': merged_pairs,\n",
    "    'word_symbols': word_symbols\n",
    "}\n",
    "with open('wordpiece_model.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_data, f, ensure_ascii=False)\n",
    "\n",
    "def wordpiece_tokenize(sentence, final_vocab):\n",
    "    tokens = gujarati_tokenize(sentence)\n",
    "    out = []\n",
    "    for word in tokens:\n",
    "        w = word.lower()\n",
    "        if len(w) == 0 or not any(ord(c) in GUJARATI_CHARS for c in w):\n",
    "            continue\n",
    "        \n",
    "        word_pieces = []\n",
    "        i = 0\n",
    "        while i < len(w):\n",
    "            matched = None\n",
    "            # try longest possible substring starting at i (greedy longest-first matching)\n",
    "            for j in range(len(w), i, -1):\n",
    "                candidate = w[i:j]\n",
    "                if candidate in final_vocab:\n",
    "                    matched = candidate\n",
    "                    break\n",
    "                elif i > 0 and f\"##{candidate}\" in final_vocab:\n",
    "                    matched = f\"##{candidate}\"\n",
    "                    break\n",
    "            \n",
    "            if matched is None:\n",
    "                # fallback: single character with ## prefix if not at start\n",
    "                if i == 0:\n",
    "                    matched = w[i]\n",
    "                else:\n",
    "                    matched = f\"##{w[i]}\"\n",
    "                i += 1\n",
    "            else:\n",
    "                # advance by actual character length (handle ## prefix correctly)\n",
    "                advance = len(matched[2:]) if matched.startswith('##') else len(matched)\n",
    "                i += advance\n",
    "            \n",
    "            word_pieces.append(matched)\n",
    "        out.extend(word_pieces)\n",
    "    return out\n",
    "\n",
    "test_tokens = wordpiece_tokenize(test_text, final_vocab)\n",
    "print(\"\\nWordPiece tokens for test sentence:\")\n",
    "print(test_tokens)\n",
    "\n",
    "print(f\"\\nFinal vocab size: {len(final_vocab)}\")\n",
    "print(f\"Number of merges: {len(merged_pairs)}\")\n",
    "print(\"Model saved to wordpiece_model.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
