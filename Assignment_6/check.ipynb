{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14db0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import json, random, math\n",
    "from typing import Iterable, Tuple, List, Dict\n",
    "\n",
    "# Config\n",
    "TOKENIZED_FILE = '../Assignment_1/tokenizer/tokenized_gu_tokens.jsonl'\n",
    "VAL_SIZE = 1000\n",
    "TEST_SIZE = 1000\n",
    "SEED = 42\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ae664b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample loaded sentence: ['àª†', 'àªµà«€àª¡àª¿àª¯à«‹', 'àªœà«àª“', ':', 'àªŠàª‚àªàª¾', 'àª®àª¾àª°à«àª•à«‡àªŸàª¯àª¾àª°à«àª¡', 'àª†àªœàª¥à«€', '25', 'àªœà«àª²àª¾àªˆ', 'àª¸à«àª§à«€', 'àª¬àª‚àª§']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import json, random\n",
    "\n",
    "MAX_SENTENCES = 1_000_000  # only first 10 lakh\n",
    "LOG_EVERY = 10_000\n",
    "\n",
    "def stream_first_sentences(path):\n",
    "    \"\"\"Yield up to 10 lakh non-empty sentences from the tokenized JSONL file.\"\"\"\n",
    "    count = 0\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f, start=1):\n",
    "            try:\n",
    "                obj = json.loads(line.strip())\n",
    "                token_blocks = obj.get(\"tokens\", [])\n",
    "                if not token_blocks:\n",
    "                    continue\n",
    "                for block in token_blocks:\n",
    "                    if block:  # non-empty sentence\n",
    "                        yield block\n",
    "                        count += 1\n",
    "                        if count % LOG_EVERY == 0:\n",
    "                            print(f\"âœ… Loaded {count:,} sentences so far...\")\n",
    "                        if count >= MAX_SENTENCES:\n",
    "                            print(f\"ğŸ›‘ Reached limit of {MAX_SENTENCES:,} sentences.\")\n",
    "                            return\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "# Test quick load (will stop early)\n",
    "sample_stream = stream_first_sentences(TOKENIZED_FILE)\n",
    "sample = next(sample_stream)\n",
    "print(\"Sample loaded sentence:\", sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56a0ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sample sentences from stream: [['àª†', 'àªµà«€àª¡àª¿àª¯à«‹', 'àªœà«àª“', ':', 'àªŠàª‚àªàª¾', 'àª®àª¾àª°à«àª•à«‡àªŸàª¯àª¾àª°à«àª¡', 'àª†àªœàª¥à«€', '25', 'àªœà«àª²àª¾àªˆ', 'àª¸à«àª§à«€', 'àª¬àª‚àª§'], ['àª†', 'àªµà«€àª¡àª¿àª¯à«‹', 'àªœà«àª“', ':', 'àªŠàª‚àªàª¾', 'àª®àª¾àª°à«àª•à«‡àªŸàª¯àª¾àª°à«àª¡', 'àª†àªœàª¥à«€', '25', 'àªœà«àª²àª¾àªˆ', 'àª¸à«àª§à«€', 'àª¬àª‚àª§'], ['àª†', 'àªµà«€àª¡àª¿àª¯à«‹', 'àªœà«àª“', ':', 'àªŠàª‚àªàª¾', 'àª®àª¾àª°à«àª•à«‡àªŸàª¯àª¾àª°à«àª¡', 'àª†àªœàª¥à«€', '25', 'àªœà«àª²àª¾àªˆ', 'àª¸à«àª§à«€', 'àª¬àª‚àª§']]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def stream_sentences(path: str) -> Iterable[List[str]]:\n",
    "    \"\"\"Stream tokenized sentences from a huge JSONL file efficiently.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                token_groups = entry.get('tokens', [])\n",
    "                if not token_groups:\n",
    "                    continue\n",
    "                for sent in token_groups:\n",
    "                    if sent and isinstance(sent, list) and len(sent) > 0:\n",
    "                        yield sent\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "# Test stream\n",
    "sample_stream = list(next(stream_sentences(TOKENIZED_FILE)) for _ in range(3))\n",
    "print(\"âœ… Sample sentences from stream:\", sample_stream)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13ed8bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Processed 10,000 sentences...\n",
      "â³ Processed 20,000 sentences...\n",
      "â³ Processed 30,000 sentences...\n",
      "â³ Processed 40,000 sentences...\n",
      "â³ Processed 50,000 sentences...\n",
      "â³ Processed 60,000 sentences...\n",
      "â³ Processed 70,000 sentences...\n",
      "â³ Processed 80,000 sentences...\n",
      "â³ Processed 90,000 sentences...\n",
      "â³ Processed 100,000 sentences...\n",
      "â³ Processed 110,000 sentences...\n",
      "â³ Processed 120,000 sentences...\n",
      "â³ Processed 130,000 sentences...\n",
      "â³ Processed 140,000 sentences...\n",
      "â³ Processed 150,000 sentences...\n",
      "â³ Processed 160,000 sentences...\n",
      "â³ Processed 170,000 sentences...\n",
      "â³ Processed 180,000 sentences...\n",
      "â³ Processed 190,000 sentences...\n",
      "â³ Processed 200,000 sentences...\n",
      "ğŸ›‘ Reached 200,000 sentences limit â€” stopping stream.\n",
      "\n",
      "âœ… Completed streaming 200,001 sentences.\n",
      "ğŸ“Š Train: 188,733, Val: 1,000, Test: 1,000\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def create_splits(path: str, val_size=1000, test_size=1000, seed=SEED, max_sentences=200_000, log_every=10_000):\n",
    "    \"\"\"\n",
    "    Create train/val/test splits using streaming & reservoir sampling.\n",
    "    - Reads up to `max_sentences`\n",
    "    - Skips blank sentences\n",
    "    - Prints progress every `log_every`\n",
    "    \"\"\"\n",
    "    val, test, train = [], [], []\n",
    "    random.seed(seed)\n",
    "    i = 0\n",
    "\n",
    "    for sent in stream_sentences(path):\n",
    "        if not sent or len(sent) == 0:\n",
    "            continue  # skip empty sentences\n",
    "\n",
    "        i += 1\n",
    "        if i % log_every == 0:\n",
    "            print(f\"â³ Processed {i:,} sentences...\")\n",
    "\n",
    "        # Limit to first `max_sentences`\n",
    "        if i > max_sentences:\n",
    "            print(f\"ğŸ›‘ Reached {max_sentences:,} sentences limit â€” stopping stream.\")\n",
    "            break\n",
    "\n",
    "        # Reservoir sampling for val/test, rest go to train\n",
    "        if len(val) < val_size:\n",
    "            val.append(sent)\n",
    "        elif random.random() < val_size / i:\n",
    "            val[random.randrange(val_size)] = sent\n",
    "        elif len(test) < test_size:\n",
    "            test.append(sent)\n",
    "        elif random.random() < test_size / i:\n",
    "            test[random.randrange(test_size)] = sent\n",
    "        else:\n",
    "            train.append(sent)\n",
    "\n",
    "    print(f\"\\nâœ… Completed streaming {i:,} sentences.\")\n",
    "    print(f\"ğŸ“Š Train: {len(train):,}, Val: {len(val):,}, Test: {len(test):,}\")\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "# Run the split creation\n",
    "train, val, test = create_splits(TOKENIZED_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8302517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 23:23:30,959 [INFO] ğŸ”¨ Building up to 4-gram counts for 200,000 sentences...\n",
      "\n",
      "2025-10-29 23:23:36,899 [INFO] ğŸ“– Loaded 10,000/200,000 sentences (5.00%)\n",
      "2025-10-29 23:23:36,902 [INFO] âœ… Processed 10,000/200,000 sentences (5.00%)\n",
      "2025-10-29 23:23:41,613 [INFO] ğŸ“– Loaded 20,000/200,000 sentences (10.00%)\n",
      "2025-10-29 23:23:41,621 [INFO] âœ… Processed 20,000/200,000 sentences (10.00%)\n",
      "2025-10-29 23:23:46,187 [INFO] ğŸ“– Loaded 30,000/200,000 sentences (15.00%)\n",
      "2025-10-29 23:23:46,277 [INFO] âœ… Processed 30,000/200,000 sentences (15.00%)\n",
      "2025-10-29 23:23:51,233 [INFO] ğŸ“– Loaded 40,000/200,000 sentences (20.00%)\n",
      "2025-10-29 23:23:51,238 [INFO] âœ… Processed 40,000/200,000 sentences (20.00%)\n",
      "2025-10-29 23:23:55,480 [INFO] ğŸ“– Loaded 50,000/200,000 sentences (25.00%)\n",
      "2025-10-29 23:23:55,484 [INFO] âœ… Processed 50,000/200,000 sentences (25.00%)\n",
      "2025-10-29 23:24:00,045 [INFO] ğŸ“– Loaded 60,000/200,000 sentences (30.00%)\n",
      "2025-10-29 23:24:00,048 [INFO] âœ… Processed 60,000/200,000 sentences (30.00%)\n",
      "2025-10-29 23:24:04,991 [INFO] ğŸ“– Loaded 70,000/200,000 sentences (35.00%)\n",
      "2025-10-29 23:24:04,994 [INFO] âœ… Processed 70,000/200,000 sentences (35.00%)\n",
      "2025-10-29 23:24:09,853 [INFO] ğŸ“– Loaded 80,000/200,000 sentences (40.00%)\n",
      "2025-10-29 23:24:09,856 [INFO] âœ… Processed 80,000/200,000 sentences (40.00%)\n",
      "2025-10-29 23:24:15,531 [INFO] ğŸ“– Loaded 90,000/200,000 sentences (45.00%)\n",
      "2025-10-29 23:24:15,533 [INFO] âœ… Processed 90,000/200,000 sentences (45.00%)\n",
      "2025-10-29 23:24:20,686 [INFO] ğŸ“– Loaded 100,000/200,000 sentences (50.00%)\n",
      "2025-10-29 23:24:20,690 [INFO] âœ… Processed 100,000/200,000 sentences (50.00%)\n",
      "2025-10-29 23:24:25,234 [INFO] ğŸ“– Loaded 110,000/200,000 sentences (55.00%)\n",
      "2025-10-29 23:24:25,241 [INFO] âœ… Processed 110,000/200,000 sentences (55.00%)\n",
      "2025-10-29 23:24:30,848 [INFO] ğŸ“– Loaded 120,000/200,000 sentences (60.00%)\n",
      "2025-10-29 23:24:30,873 [INFO] âœ… Processed 120,000/200,000 sentences (60.00%)\n",
      "2025-10-29 23:24:36,956 [INFO] ğŸ“– Loaded 130,000/200,000 sentences (65.00%)\n",
      "2025-10-29 23:24:36,961 [INFO] âœ… Processed 130,000/200,000 sentences (65.00%)\n",
      "2025-10-29 23:24:42,049 [INFO] ğŸ“– Loaded 140,000/200,000 sentences (70.00%)\n",
      "2025-10-29 23:24:42,056 [INFO] âœ… Processed 140,000/200,000 sentences (70.00%)\n",
      "2025-10-29 23:24:54,866 [INFO] ğŸ“– Loaded 150,000/200,000 sentences (75.00%)\n",
      "2025-10-29 23:24:55,897 [INFO] âœ… Processed 150,000/200,000 sentences (75.00%)\n",
      "2025-10-29 23:25:02,056 [INFO] ğŸ“– Loaded 160,000/200,000 sentences (80.00%)\n",
      "2025-10-29 23:25:02,059 [INFO] âœ… Processed 160,000/200,000 sentences (80.00%)\n",
      "2025-10-29 23:25:08,588 [INFO] ğŸ“– Loaded 170,000/200,000 sentences (85.00%)\n",
      "2025-10-29 23:25:08,591 [INFO] âœ… Processed 170,000/200,000 sentences (85.00%)\n",
      "2025-10-29 23:25:13,223 [INFO] ğŸ“– Loaded 180,000/200,000 sentences (90.00%)\n",
      "2025-10-29 23:25:13,229 [INFO] âœ… Processed 180,000/200,000 sentences (90.00%)\n",
      "2025-10-29 23:25:19,071 [INFO] ğŸ“– Loaded 190,000/200,000 sentences (95.00%)\n",
      "2025-10-29 23:25:19,138 [INFO] âœ… Processed 190,000/200,000 sentences (95.00%)\n",
      "2025-10-29 23:25:30,797 [INFO] ğŸ“– Loaded 200,000/200,000 sentences (100.00%)\n",
      "2025-10-29 23:25:30,799 [INFO] âœ… Processed 200,000/200,000 sentences (100.00%)\n",
      "2025-10-29 23:25:30,805 [INFO] âœ… Finished streaming (yielded 200,000 sentences).\n",
      "2025-10-29 23:25:30,808 [INFO] ğŸ¯ N-gram counting complete after 200,000 sentences.\n",
      "2025-10-29 23:26:23,066 [INFO] ğŸ§© 1-gram unique count: 393,430\n",
      "2025-10-29 23:26:23,071 [INFO] ğŸ§© 2-gram unique count: 3,562,996\n",
      "2025-10-29 23:26:23,075 [INFO] ğŸ§© 3-gram unique count: 6,494,457\n",
      "2025-10-29 23:26:23,078 [INFO] ğŸ§© 4-gram unique count: 7,803,608\n",
      "2025-10-29 23:26:23,080 [INFO] ğŸ“Š Total unigram tokens: 9,475,762\n",
      "2025-10-29 23:26:23,083 [INFO] ğŸ“š Vocabulary size: 393,430\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import json\n",
    "import logging\n",
    "from collections import Counter\n",
    "from typing import List, Iterable, Any\n",
    "\n",
    "# ================================\n",
    "# CONFIG\n",
    "# ================================\n",
    "TOKENIZED_FILE = \"../Assignment_1/tokenizer/tokenized_gu_tokens.jsonl\"\n",
    "MAX_SENTENCES = 200_000     # process only 2 lakh sentences\n",
    "MAX_N = 4                   # up to 4-grams (quadrigrams)\n",
    "LOG_EVERY = 10_000          # log every 10k sentences\n",
    "\n",
    "# ================================\n",
    "# LOGGING SETUP\n",
    "# ================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"ngram_build.log\", mode=\"w\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ================================\n",
    "# HELPERS\n",
    "# ================================\n",
    "def flatten_tokens(obj: Any, out: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Flatten a nested list/tuple structure into `out`.\n",
    "    Supports arbitrary nesting depth for safety (but typical data is shallow).\n",
    "    \"\"\"\n",
    "    if obj is None:\n",
    "        return\n",
    "    if isinstance(obj, (str,)):\n",
    "        out.append(obj)\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        for item in obj:\n",
    "            flatten_tokens(item, out)\n",
    "    else:\n",
    "        # ignore non-str, non-list items\n",
    "        try:\n",
    "            out.append(str(obj))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def stream_first_sentences(file_path: str, max_sentences: int) -> Iterable[List[str]]:\n",
    "    \"\"\"\n",
    "    Yield flattened token lists (one list per sentence) up to max_sentences.\n",
    "    Handles records where 'tokens' field can be:\n",
    "      - a flat list of tokens\n",
    "      - a list of lists (multiple token lists)\n",
    "      - nested lists (rare)\n",
    "    Skips empty/malformed entries.\n",
    "    \"\"\"\n",
    "    yielded = 0\n",
    "    with open(file_path, 'r', encoding='utf-8') as fh:\n",
    "        for lineno, line in enumerate(fh, start=1):\n",
    "            if yielded >= max_sentences:\n",
    "                break\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                # skip malformed JSON line\n",
    "                if lineno % 100_000 == 0:\n",
    "                    logger.warning(f\"Skipped malformed JSON at line {lineno}\")\n",
    "                continue\n",
    "\n",
    "            # Expected shape: {'tokens': [...] } or sometimes a raw list\n",
    "            tokens_field = None\n",
    "            if isinstance(obj, dict) and 'tokens' in obj:\n",
    "                tokens_field = obj['tokens']\n",
    "            elif isinstance(obj, list):\n",
    "                tokens_field = obj\n",
    "            else:\n",
    "                # skip unknown record shapes\n",
    "                continue\n",
    "\n",
    "            # Flatten token structure into a single list of strings\n",
    "            flat = []\n",
    "            flatten_tokens(tokens_field, flat)\n",
    "\n",
    "            # skip empty after flattening\n",
    "            if not flat:\n",
    "                continue\n",
    "\n",
    "            yielded += 1\n",
    "            if yielded % LOG_EVERY == 0:\n",
    "                logger.info(f\"ğŸ“– Loaded {yielded:,}/{max_sentences:,} sentences ({yielded/max_sentences*100:.2f}%)\")\n",
    "            yield flat\n",
    "\n",
    "    logger.info(f\"âœ… Finished streaming (yielded {yielded:,} sentences).\")\n",
    "\n",
    "def ngrams_from_sentence(tokens: List[str], n: int):\n",
    "    \"\"\"Yield n-gram tuples from a token list.\"\"\"\n",
    "    if not tokens or n <= 0:\n",
    "        return\n",
    "    L = len(tokens)\n",
    "    for i in range(L - n + 1):\n",
    "        # slice returns list -> wrap as tuple so it's hashable\n",
    "        yield tuple(tokens[i:i+n])\n",
    "\n",
    "def build_ngram_counts(sent_stream: Iterable[List[str]], max_n: int = 4, log_every: int = 10_000):\n",
    "    \"\"\"\n",
    "    Build n-gram counts for n in [1..max_n].\n",
    "    Returns (counters, total_unigrams, vocab_set)\n",
    "    \"\"\"\n",
    "    counters = {n: Counter() for n in range(1, max_n + 1)}\n",
    "    vocab = set()\n",
    "    total_unigrams = 0\n",
    "    processed = 0\n",
    "\n",
    "    for processed, sent in enumerate(sent_stream, start=1):\n",
    "        # update vocab and unigram total\n",
    "        for t in sent:\n",
    "            vocab.add(t)\n",
    "        total_unigrams += len(sent)\n",
    "\n",
    "        # update n-gram counters\n",
    "        for n in range(1, max_n + 1):\n",
    "            for ng in ngrams_from_sentence(sent, n):\n",
    "                counters[n][ng] += 1\n",
    "\n",
    "        if processed % log_every == 0:\n",
    "            logger.info(f\"âœ… Processed {processed:,}/{MAX_SENTENCES:,} sentences ({processed/MAX_SENTENCES*100:.2f}%)\")\n",
    "\n",
    "    logger.info(f\"ğŸ¯ N-gram counting complete after {processed:,} sentences.\")\n",
    "    return counters, total_unigrams, vocab\n",
    "\n",
    "# ================================\n",
    "# RUN: build counts\n",
    "# ================================\n",
    "logger.info(f\"ğŸ”¨ Building up to {MAX_N}-gram counts for {MAX_SENTENCES:,} sentences...\\n\")\n",
    "sentence_stream = stream_first_sentences(TOKENIZED_FILE, MAX_SENTENCES)\n",
    "counts, total_unigrams, vocab = build_ngram_counts(sentence_stream, max_n=MAX_N, log_every=LOG_EVERY)\n",
    "\n",
    "# Summary\n",
    "logger.info(f\"ğŸ§© 1-gram unique count: {len(counts[1]):,}\")\n",
    "logger.info(f\"ğŸ§© 2-gram unique count: {len(counts[2]):,}\")\n",
    "logger.info(f\"ğŸ§© 3-gram unique count: {len(counts[3]):,}\")\n",
    "logger.info(f\"ğŸ§© 4-gram unique count: {len(counts[4]):,}\")\n",
    "logger.info(f\"ğŸ“Š Total unigram tokens: {total_unigrams:,}\")\n",
    "logger.info(f\"ğŸ“š Vocabulary size: {len(vocab):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55e8e622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 23:43:45,530 [INFO] Using vocabulary size = 393430, total unigrams = 9,475,762\n",
      "2025-10-29 23:43:46,773 [INFO] Total counts per order: 1:9,475,762, 2:9,275,762, 3:9,075,901, 4:8,878,254\n",
      "2025-10-29 23:43:59,754 [INFO] Total bigram types: 3,562,996\n",
      "2025-10-29 23:55:22,636 [INFO] Precomputations done: continuation_unigram (len)=384580, Nplus keys: 384580/3534602/6425960\n",
      "2025-10-29 23:55:23,949 [INFO] Candidate vocabulary size for generation: 5000\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import math\n",
    "import logging\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---- Config for generation ----\n",
    "CANDIDATE_VOCAB_SIZE = 5000   # use top 5k unigrams as candidates during generation (pruning)\n",
    "BEAM_SIZE = 20\n",
    "MAX_GEN_LEN = 20\n",
    "\n",
    "# ---- Quick sanity-checks ----\n",
    "assert 1 in counts and 2 in counts and 3 in counts and 4 in counts, \"counts[1..4] required\"\n",
    "\n",
    "# ---- Totals ----\n",
    "total_unigram_tokens = total_unigrams  # from earlier\n",
    "vocab_size = len(vocab)\n",
    "logger.info(f\"Using vocabulary size = {vocab_size}, total unigrams = {total_unigram_tokens:,}\")\n",
    "\n",
    "# ---- Precompute totals for each n ----\n",
    "counts_total = {n: sum(counts[n].values()) for n in range(1,5)}\n",
    "logger.info(\"Total counts per order: \" + \", \".join(f\"{n}:{counts_total[n]:,}\" for n in counts_total))\n",
    "\n",
    "# ---- Precompute continuation counts needed for Kneser-Ney ----\n",
    "# continuation_unigram[w] = number of unique bigram contexts (*, w)\n",
    "continuation_unigram = Counter()\n",
    "for (w1, w2), c in counts[2].items():\n",
    "    continuation_unigram[w2] += 1\n",
    "\n",
    "total_bigram_types = len(counts[2])\n",
    "logger.info(f\"Total bigram types: {total_bigram_types:,}\")\n",
    "\n",
    "# For trigram and 4-gram continuation computations, we need:\n",
    "#   N1+(history) = number of unique words that follow that history\n",
    "unique_continuations = {n: defaultdict(set) for n in (1,2,3)}  # history -> set of continuations\n",
    "# For bigram history (w2) we want unique words that follow w2 in (w2,w3) ... etc.\n",
    "\n",
    "# Build for histories of length 1 (for bigram histories), 2, 3\n",
    "for (w1, w2), _ in counts[2].items():\n",
    "    unique_continuations[1][(w2,)].add(w1)  # not used directly but safe\n",
    "\n",
    "for (w1, w2, w3), _ in counts[3].items():\n",
    "    hist2 = (w1, w2)\n",
    "    unique_continuations[2][hist2].add(w3)\n",
    "\n",
    "for (w1, w2, w3, w4), _ in counts[4].items():\n",
    "    hist3 = (w1, w2, w3)\n",
    "    unique_continuations[3][hist3].add(w4)\n",
    "\n",
    "# Convert sets -> counts (N+)\n",
    "Nplus = {1: {}, 2: {}, 3: {}}\n",
    "for n in (1,2,3):\n",
    "    for h, s in unique_continuations[n].items():\n",
    "        Nplus[n][h] = len(s)\n",
    "\n",
    "logger.info(\"Precomputations done: continuation_unigram (len)=%d, Nplus keys: %d/%d/%d\",\n",
    "            len(continuation_unigram), len(Nplus[1]), len(Nplus[2]), len(Nplus[3]))\n",
    "\n",
    "# ---- Top candidate vocab for generation (pruning) based on unigram frequency ----\n",
    "top_unigrams = [w for (w,), _ in counts[1].most_common(CANDIDATE_VOCAB_SIZE)]\n",
    "logger.info(\"Candidate vocabulary size for generation: %d\", len(top_unigrams))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0528f729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 00:00:02,009 [INFO] Modified Kneser-Ney functions ready.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from functools import lru_cache\n",
    "\n",
    "# Discount value used in modified Kneser-Ney\n",
    "D = 0.75\n",
    "\n",
    "# Precompute sums for lower orders used by KN\n",
    "total_bigram_types = len(counts[2])\n",
    "\n",
    "# Kneser-Ney unigram continuation prob\n",
    "@lru_cache(maxsize=500000)\n",
    "def kn_unigram_continuation_prob(w):\n",
    "    # P_continuation(w) = #contexts that w appears in (as last in bigram) / total number of bigram types\n",
    "    return continuation_unigram.get(w, 0) / total_bigram_types if total_bigram_types > 0 else 0.0\n",
    "\n",
    "# recursive KN: P(w4 | w1 w2 w3)\n",
    "@lru_cache(maxsize=500000)\n",
    "def kn_prob_4(w1, w2, w3, w4):\n",
    "    # counts\n",
    "    hist3 = (w1, w2, w3)\n",
    "    c3 = counts[3].get(hist3, 0)\n",
    "    c4 = counts[4].get((w1,w2,w3,w4), 0)\n",
    "\n",
    "    # If history count > 0 use formula; else back off\n",
    "    if c3 > 0:\n",
    "        # first term\n",
    "        term1 = max(c4 - D, 0) / c3\n",
    "\n",
    "        # lambda = D * (number of unique continuations of hist3) / c3\n",
    "        N1plus = Nplus[3].get(hist3, 0)\n",
    "        lamb = (D * N1plus) / c3 if c3>0 else 0.0\n",
    "\n",
    "        # lower-order (3-gram) probability P_kn(w4 | w2 w3)\n",
    "        lower = kn_prob_3(w2, w3, w4)\n",
    "        return term1 + lamb * lower\n",
    "    else:\n",
    "        # backoff to 3-gram\n",
    "        return kn_prob_3(w2, w3, w4)\n",
    "\n",
    "@lru_cache(maxsize=500000)\n",
    "def kn_prob_3(w1, w2, w3):\n",
    "    hist2 = (w1, w2)\n",
    "    c2 = counts[2].get(hist2, 0)\n",
    "    c3 = counts[3].get((w1,w2,w3), 0)\n",
    "    if c2 > 0:\n",
    "        term1 = max(c3 - D, 0) / c2\n",
    "        N1plus = Nplus[2].get(hist2, 0)\n",
    "        lamb = (D * N1plus) / c2\n",
    "        lower = kn_prob_2(w2, w3)\n",
    "        return term1 + lamb * lower\n",
    "    else:\n",
    "        return kn_prob_2(w2, w3)\n",
    "\n",
    "@lru_cache(maxsize=500000)\n",
    "def kn_prob_2(w1, w2):\n",
    "    hist1 = (w1,)\n",
    "    c1 = counts[1].get(hist1, 0)  # note: counts[1] keys are (w,), so hist1=(w1,)\n",
    "    c2 = counts[2].get((w1,w2), 0)\n",
    "    if c1 > 0:\n",
    "        term1 = max(c2 - D, 0) / c1\n",
    "        N1plus = Nplus[1].get(hist1, 0)\n",
    "        lamb = (D * N1plus) / c1 if c1>0 else 0.0\n",
    "        lower = kn_unigram_continuation_prob(w2)  # final lower\n",
    "        return term1 + lamb * lower\n",
    "    else:\n",
    "        return kn_unigram_continuation_prob(w2)\n",
    "\n",
    "logger.info(\"Modified Kneser-Ney functions ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45626e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 00:00:23,690 [INFO] Katz Backoff functions (approximate) ready.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from functools import lru_cache\n",
    "\n",
    "# Katz discount value\n",
    "KATZ_D = 0.5\n",
    "\n",
    "# Precompute lower-order MLE (add-one smoothed) probabilities for normalization\n",
    "unigram_total = counts_total[1]\n",
    "def mle_unigram_prob(w):\n",
    "    # add-one smoothing\n",
    "    return (counts[1].get((w,), 0) + 1) / (unigram_total + vocab_size)\n",
    "\n",
    "@lru_cache(maxsize=500000)\n",
    "def katz_prob_4(w1,w2,w3,w4):\n",
    "    \"\"\"\n",
    "    Approximate Katz backoff probability for quadrigram:\n",
    "      - If seen: (c-d)/c(hist)\n",
    "      - Else: backoff weight * Katz prob of lower order (trigram)\n",
    "    We compute backoff weight alpha(hist) from leftover mass and lower-order sum.\n",
    "    \"\"\"\n",
    "    quad = (w1,w2,w3,w4)\n",
    "    tri_hist = (w1,w2,w3)\n",
    "    c_tri = counts[3].get(tri_hist, 0)\n",
    "    c_quad = counts[4].get(quad, 0)\n",
    "\n",
    "    if c_tri > 0 and c_quad > 0:\n",
    "        return max(c_quad - KATZ_D, 0) / c_tri\n",
    "\n",
    "    # compute alpha for tri_hist\n",
    "    # sum_seen = sum((c - d)/c_tri for each seen continuation)\n",
    "    seen_conts = [w4_ for (a,b,c,d), cval in counts[4].items() if (a,b,c) == tri_hist]\n",
    "    if c_tri == 0:\n",
    "        # backoff to trigram MLE\n",
    "        return katz_prob_3(w2,w3,w4)\n",
    "    sum_seen = 0.0\n",
    "    sum_lower_seen = 0.0\n",
    "    for cont in seen_conts:\n",
    "        c_q = counts[4].get((tri_hist[0],tri_hist[1],tri_hist[2],cont),0)\n",
    "        sum_seen += max(c_q - KATZ_D, 0) / c_tri\n",
    "        # accumulate lower-order prob for seen cont\n",
    "        sum_lower_seen += katz_prob_3(tri_hist[1], tri_hist[2], cont)\n",
    "\n",
    "    leftover = max(1.0 - sum_seen, 0.0)\n",
    "    # lower_total = sum over all vocab of lower_prob; should be 1.0\n",
    "    lower_total = 1.0\n",
    "    lower_unseen_mass = max(lower_total - sum_lower_seen, 1e-12)\n",
    "    alpha = leftover / lower_unseen_mass\n",
    "    return alpha * katz_prob_3(w2,w3,w4)\n",
    "\n",
    "@lru_cache(maxsize=500000)\n",
    "def katz_prob_3(w1,w2,w3):\n",
    "    tri = (w1,w2,w3)\n",
    "    bi_hist = (w1,w2)\n",
    "    c_bi = counts[2].get(bi_hist, 0)\n",
    "    c_tri = counts[3].get(tri, 0)\n",
    "    if c_bi > 0 and c_tri > 0:\n",
    "        return max(c_tri - KATZ_D, 0) / c_bi\n",
    "    # else backoff:\n",
    "    # compute alpha for bi_hist similarly\n",
    "    seen_conts = [t3 for (a,b,c), cval in counts[3].items() if (a,b)==bi_hist]\n",
    "    if c_bi == 0:\n",
    "        return mle_unigram_prob(w3)\n",
    "    sum_seen = 0.0\n",
    "    sum_lower_seen = 0.0\n",
    "    for cont in seen_conts:\n",
    "        c_t = counts[3].get((bi_hist[0],bi_hist[1],cont),0)\n",
    "        sum_seen += max(c_t - KATZ_D, 0) / c_bi\n",
    "        sum_lower_seen += mle_unigram_prob(cont)\n",
    "    leftover = max(1.0 - sum_seen, 0.0)\n",
    "    lower_unseen_mass = max(1.0 - sum_lower_seen, 1e-12)\n",
    "    alpha = leftover / lower_unseen_mass\n",
    "    return alpha * mle_unigram_prob(w3)\n",
    "\n",
    "logger.info(\"Katz Backoff functions (approximate) ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4d07eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 00:00:53,305 [INFO] Generating 3 greedy sentences (Kneser-Ney):\n",
      "2025-10-30 00:00:53,435 [INFO] Greedy generation start: . àª›à«‡ ,\n",
      "2025-10-30 00:01:33,349 [INFO] Greedy generated (8): . àª›à«‡ , àª…àª¨à«‡ àª¤à«‡ àªªàª£ àªàª• àª®à«‹àªŸà«‹ ...\n",
      "2025-10-30 00:01:37,865 [INFO] Greedy generation start: . àª›à«‡ ,\n",
      "2025-10-30 00:01:37,902 [INFO] Greedy generated (8): . àª›à«‡ , àª…àª¨à«‡ àª¤à«‡ àªªàª£ àªàª• àª®à«‹àªŸà«‹ ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GREEDY: . àª›à«‡ , àª…àª¨à«‡ àª¤à«‡ àªªàª£ àªàª• àª®à«‹àªŸà«‹ àª¸àªµàª¾àª² àª›à«‡ .\n",
      "GREEDY: . àª›à«‡ , àª…àª¨à«‡ àª¤à«‡ àªªàª£ àªàª• àª®à«‹àªŸà«‹ àª¸àªµàª¾àª² àª›à«‡ .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 00:01:38,043 [INFO] Greedy generation start: . àª›à«‡ ,\n",
      "2025-10-30 00:01:38,097 [INFO] Greedy generated (8): . àª›à«‡ , àª…àª¨à«‡ àª¤à«‡ àªªàª£ àªàª• àª®à«‹àªŸà«‹ ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GREEDY: . àª›à«‡ , àª…àª¨à«‡ àª¤à«‡ àªªàª£ àªàª• àª®à«‹àªŸà«‹ àª¸àªµàª¾àª² àª›à«‡ .\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def generate_greedy(start_tokens=None, max_len=MAX_GEN_LEN, model='kneser'):\n",
    "    if start_tokens is None:\n",
    "        # start with three most frequent unigrams (or less)\n",
    "        start_tokens = [w for (w,), _ in counts[1].most_common(3)]\n",
    "    sent = list(start_tokens[:3])  # ensure at least 3 tokens\n",
    "    logger.info(\"Greedy generation start: \" + \" \".join(sent))\n",
    "\n",
    "    for step in range(max_len - len(sent)):\n",
    "        best_word = None\n",
    "        best_score = -float('inf')\n",
    "        # candidate pruning\n",
    "        candidates = top_unigrams  # list of words (strings)\n",
    "        for w in candidates:\n",
    "            if model == 'kneser':\n",
    "                p = kn_prob_4(sent[-3], sent[-2], sent[-1], w)\n",
    "            elif model == 'katz':\n",
    "                p = katz_prob_4(sent[-3], sent[-2], sent[-1], w)\n",
    "            else:\n",
    "                # default to KN\n",
    "                p = kn_prob_4(sent[-3], sent[-2], sent[-1], w)\n",
    "            # use log-score\n",
    "            if p <= 0:\n",
    "                score = -1e300\n",
    "            else:\n",
    "                score = math.log(p)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_word = w\n",
    "        sent.append(best_word)\n",
    "        if (step+1) % 5 == 0:\n",
    "            logger.info(f\"Greedy generated ({len(sent)}): {' '.join(sent[:10])} ...\")\n",
    "        if best_word in ['.', 'à¥¤']:\n",
    "            break\n",
    "    return \" \".join(sent)\n",
    "\n",
    "# Example\n",
    "logger.info(\"Generating 3 greedy sentences (Kneser-Ney):\")\n",
    "for i in range(3):\n",
    "    print(\"GREEDY:\", generate_greedy(model='kneser'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6ecceaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 00:10:12,178 [INFO] Generating 2 beam-search sentences (beam=20, Kneser-Ney):\n",
      "2025-10-30 00:10:12,957 [INFO] Beam step 0: top score -1.59, seq head: . àª›à«‡ , àª…àª¨à«‡...\n",
      "2025-10-30 00:10:29,341 [INFO] Beam step 2: top score -5.38, seq head: . àª›à«‡ , àª¤à«‹ àªªàª›à«€ àª¤àª®à«‡...\n",
      "2025-10-30 00:10:32,896 [INFO] Beam step 4: top score -7.36, seq head: . àª›à«‡ , àª…àª¨à«‡ àª¤à«‡ àªœ àª¸àª®àª¯à«‡ ,...\n",
      "2025-10-30 00:10:36,440 [INFO] Beam step 6: top score -8.90, seq head: . àª›à«‡ , àª›à«‹àª¡ àªªàª°àª¨àª¾ àªªàª¾àª‚àª¦àª¡àª¾ àªªàª¾àª¤àª³àª¾ àª¹à«‹àª¯...\n",
      "2025-10-30 00:10:40,489 [INFO] Beam step 8: top score -12.58, seq head: . àª›à«‡ , àª›à«‹àª¡ àªªàª°àª¨àª¾ àªªàª¾àª‚àª¦àª¡àª¾ àªªàª¾àª¤àª³àª¾ àª¹à«‹àª¯...\n",
      "2025-10-30 00:10:42,540 [INFO] Beam step 0: top score -1.59, seq head: . àª›à«‡ , àª…àª¨à«‡...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEAM: . àª›à«‡ , àª…àª¨à«‡ àª¤à«‡àª¥à«€ àªªàª° . àª¡à«€ . àª¸à«€ . àª¬à«€ .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 00:19:22,380 [INFO] Beam step 2: top score -5.38, seq head: . àª›à«‡ , àª¤à«‹ àªªàª›à«€ àª¤àª®à«‡...\n",
      "2025-10-30 00:19:44,242 [INFO] Beam step 4: top score -7.36, seq head: . àª›à«‡ , àª…àª¨à«‡ àª¤à«‡ àªœ àª¸àª®àª¯à«‡ ,...\n",
      "2025-10-30 00:19:50,826 [INFO] Beam step 6: top score -8.90, seq head: . àª›à«‡ , àª›à«‹àª¡ àªªàª°àª¨àª¾ àªªàª¾àª‚àª¦àª¡àª¾ àªªàª¾àª¤àª³àª¾ àª¹à«‹àª¯...\n",
      "2025-10-30 00:19:59,974 [INFO] Beam step 8: top score -12.58, seq head: . àª›à«‡ , àª›à«‹àª¡ àªªàª°àª¨àª¾ àªªàª¾àª‚àª¦àª¡àª¾ àªªàª¾àª¤àª³àª¾ àª¹à«‹àª¯...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEAM: . àª›à«‡ , àª…àª¨à«‡ àª¤à«‡àª¥à«€ àªªàª° . àª¡à«€ . àª¸à«€ . àª¬à«€ .\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import heapq\n",
    "\n",
    "def generate_beam(start_tokens=None, beam_size=BEAM_SIZE, max_len=MAX_GEN_LEN, model='kneser'):\n",
    "    if start_tokens is None:\n",
    "        start_tokens = [w for (w,), _ in counts[1].most_common(3)]\n",
    "    initial = list(start_tokens[:3])\n",
    "    # beams: list of tuples (score, seq)\n",
    "    beams = [(0.0, initial)]\n",
    "    for step in range(max_len - len(initial)):\n",
    "        new_beams = []\n",
    "        for score, seq in beams:\n",
    "            # candidate pruning\n",
    "            for w in top_unigrams:\n",
    "                if model == 'kneser':\n",
    "                    p = kn_prob_4(seq[-3], seq[-2], seq[-1], w)\n",
    "                elif model == 'katz':\n",
    "                    p = katz_prob_4(seq[-3], seq[-2], seq[-1], w)\n",
    "                else:\n",
    "                    p = kn_prob_4(seq[-3], seq[-2], seq[-1], w)\n",
    "                if p <= 0:\n",
    "                    continue\n",
    "                new_seq = seq + [w]\n",
    "                new_score = score + math.log(p)\n",
    "                new_beams.append((new_score, new_seq))\n",
    "        if not new_beams:\n",
    "            break\n",
    "        # keep top beam_size by score (higher is better)\n",
    "        new_beams.sort(key=lambda x: x[0], reverse=True)\n",
    "        beams = new_beams[:beam_size]\n",
    "        if step % 2 == 0:\n",
    "            logger.info(f\"Beam step {step}: top score {beams[0][0]:.2f}, seq head: {' '.join(beams[0][1][:8])}...\")\n",
    "        # stop early if best beam ends with sentence-end token\n",
    "        if beams and beams[0][1][-1] in ['.', 'à¥¤']:\n",
    "            break\n",
    "    best = max(beams, key=lambda x: x[0])\n",
    "    return \" \".join(best[1])\n",
    "\n",
    "# Example\n",
    "logger.info(\"Generating 2 beam-search sentences (beam=20, Kneser-Ney):\")\n",
    "for i in range(2):\n",
    "    print(\"BEAM:\", generate_beam(model='kneser', beam_size=20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
